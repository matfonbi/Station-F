# -*- coding: utf-8 -*-
"""Station F.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AWxbB38hv5vjLiMgCdbOINwIA1gK49_y

# **PROJET STATION F**
"""

# ======================================================
#  ‚öôÔ∏è Pr√©paration compl√®te des donn√©es - Station F / Colab
#  Version am√©lior√©e :
#   - ignore les exp√©riences sans date
#   - ne met plus 0.0 si dur√©e inconnue ‚Üí NaN
# ======================================================

import json
import pandas as pd
import numpy as np
import re
from datetime import datetime
from google.colab import files

# --- 1. Charger le fichier ---
DATA_PATH = "data/data_train.json"
# --- 2. Liste globale pour auditer les dur√©es non reconnues ---
unrecognized_durations = []

# --- 3. Fonctions utilitaires ---

def extract_highest_degree_info(diplomas):
    """Retourne le niveau et le domaine du dipl√¥me le plus √©lev√©."""
    if not diplomas:
        return "Unknown", "Unknown"

    levels_order = ["Certificat", "Licence", "Master", "Doctorat"]
    best_level_idx = -1
    best_diploma = None

    for d in diplomas:
        level = d.get("level", "Unknown")
        if level in levels_order:
            idx = levels_order.index(level)
            if idx > best_level_idx:
                best_level_idx = idx
                best_diploma = d

    if best_diploma:
        return (
            best_diploma.get("level", "Unknown"),
            best_diploma.get("title", "Unknown")
        )
    else:
        return "Unknown", "Unknown"


def parse_duration_to_years(exp):
    """
    Retourne la dur√©e en ann√©es pour une exp√©rience individuelle.
    Essaie les formats : start_date/end_date, duration, dates.
    Renvoie np.nan si non reconnu ou non applicable.
    """
    now = pd.Timestamp.now()

    # V√©rifier qu‚Äôil existe au moins une info temporelle
    if not any(k in exp for k in ["start_date", "end_date", "duration", "dates"]):
        return np.nan  # pas de dur√©e du tout ‚Üí on ignore

    # 1Ô∏è‚É£ Cas : start_date / end_date
    if "start_date" in exp and "end_date" in exp:
        start = pd.to_datetime(exp.get("start_date"), errors="coerce")
        end = exp.get("end_date")

        if isinstance(end, str) and re.search(r"pr√©sent|present", end, re.IGNORECASE):
            end = now
        else:
            end = pd.to_datetime(end, errors="coerce")

        if pd.notna(start) and pd.notna(end):
            return round((end - start).days / 365, 2)

    # 2Ô∏è‚É£ Cas : champ unique "duration"
    elif "duration" in exp and exp["duration"]:
        text = str(exp["duration"]).lower().strip()

        # Exemples : "6 mois", "5 ans", "1 an", "2015 - Pr√©sent"
        if re.search(r"\d+\s*mois", text):
            months = int(re.search(r"\d+", text).group())
            return round(months / 12, 2)

        elif re.search(r"\d+\s*an", text):
            years = int(re.search(r"\d+", text).group())
            return float(years)

        elif re.search(r"\d{4}", text):
            years = re.findall(r"\d{4}", text)
            if len(years) == 2:
                return int(years[1]) - int(years[0])
            elif len(years) == 1 and re.search(r"pr√©sent|present", text):
                return now.year - int(years[0])

    # 3Ô∏è‚É£ Cas : champ unique "dates"
    elif "dates" in exp and exp["dates"]:
        text = str(exp["dates"]).lower().strip()

        if re.search(r"\d{4}", text):
            years = re.findall(r"\d{4}", text)
            if len(years) == 2:
                return int(years[1]) - int(years[0])
            elif len(years) == 1 and re.search(r"pr√©sent|present", text):
                return now.year - int(years[0])

    # ‚ùå Cas non reconnu malgr√© une donn√©e
    if any(v for v in exp.values()):
        unrecognized_durations.append(exp)
    return np.nan


def total_experience_years(experiences):
    """
    Calcule la dur√©e cumul√©e (en ann√©es) de toutes les exp√©riences d‚Äôun formateur.
    Retourne NaN si aucune dur√©e exploitable n‚Äôest trouv√©e.
    """
    if not experiences:
        return np.nan  # ‚ö†Ô∏è ne pas consid√©rer comme 0 an

    durations = [parse_duration_to_years(exp) for exp in experiences]
    durations = [d for d in durations if pd.notna(d)]

    if not durations:
        return np.nan  # ‚ö†Ô∏è aucune info exploitable

    return round(sum(durations), 2)


def count_experiences(experiences):
    """Compte le nombre d'exp√©riences professionnelles."""
    return len(experiences) if experiences else 0


def mean_past_rating(pastCourses):
    """Calcule la moyenne des notes pass√©es."""
    if not pastCourses:
        return np.nan
    return np.mean([c.get("numberOfStars", np.nan) for c in pastCourses])


def flatten_entry(entry):
    """Aplati un professeur en plusieurs lignes (une par cours)."""
    flattened = []
    highest_level, highest_domain = extract_highest_degree_info(entry.get("diplomas"))
    total_years = total_experience_years(entry.get("experiences"))  # ‚úÖ uniquement sur experiences

    for course in entry.get("pastCourses", []):
        flattened.append({
            "firstname": entry.get("fistname") or entry.get("firstname"),
            "lastname": entry.get("lastname"),
            "city": entry.get("city", "Unknown"),
            "description": entry.get("description", ""),
            "highest_degree": highest_level,
            "highest_degree_domain": highest_domain,
            "num_diplomas": len(entry.get("diplomas", [])),
            "num_experiences": count_experiences(entry.get("experiences")),
            "total_experience_years": total_years,  # üÜï NaN si inconnu, pas 0
            "mean_past_rating": mean_past_rating(entry.get("pastCourses")),
            "course_title": course.get("title", "Unknown"),
            "course_rating": course.get("numberOfStars", np.nan)
        })
    return flattened


# --- 4. Charger et transformer les donn√©es ---
with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

rows = []
for teacher in data:
    rows.extend(flatten_entry(teacher))

df = pd.DataFrame(rows)

# --- 5. Nettoyage rapide ---
df.dropna(subset=["course_rating"], inplace=True)
df.reset_index(drop=True, inplace=True)

# --- 6. R√©sum√© + audit ---
print(f"Nombre total de cours : {len(df)}")
print(f"Nombre d'exp√©riences non reconnues : {len(unrecognized_durations)}")

if unrecognized_durations:
    print("\nüîç Exemple d‚Äôexp√©rience non reconnue :")
    for e in unrecognized_durations[:3]:
        print(e)

print("\n‚úÖ Exemple de lignes format√©es :")
df.head(10)

# ======================================================
#  ‚úÇÔ∏è Split du jeu de donn√©es : train / test
# ======================================================

from sklearn.model_selection import train_test_split

# --- 1. D√©finir la cible et les features ---
y = df["course_rating"]
X = df.drop(columns=["course_rating", "firstname", "lastname"])

# --- 2. D√©coupage train / test ---
# 80% pour l'entra√Ænement, 20% pour le test
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42
)

# --- 3. V√©rification des dimensions ---
print(f"üß© Taille totale : {df.shape[0]} √©chantillons")
print(f"üìò Entra√Ænement : {X_train.shape[0]} ({100 * X_train.shape[0] / df.shape[0]:.1f}%)")
print(f"üìô Test : {X_test.shape[0]} ({100 * X_test.shape[0] / df.shape[0]:.1f}%)")

# --- 4. Aper√ßu rapide ---
print("\n‚úÖ Exemple d'une ligne de X_train :")
display(X_train.head(10))

print("\nüéØ Exemple des valeurs cibles (y_train) :")
display(y_train.head(5))

# Test d'adaptation aux incconus


# ======================================================
#  üí™ Version robuste du pipeline - Support explicite de "Inconnu"
# ======================================================

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from nltk.corpus import stopwords
import nltk
import random

nltk.download("stopwords")
french_stopwords = stopwords.words("french")

# --- 1Ô∏è‚É£ Fonction pour injecter des "Inconnu" al√©atoirement ---
def simulate_unknowns(df, p=0.1):
    """
    Injecte des valeurs 'Inconnu' ou NaN dans un DataFrame
    pour habituer le mod√®le √† g√©rer des champs manquants.
    """
    df_sim = df.copy()
    for col in df_sim.columns:
        mask = np.random.rand(len(df_sim)) < p
        if df_sim[col].dtype == object:
            df_sim.loc[mask, col] = "Inconnu"
        else:
            df_sim.loc[mask, col] = np.nan
    return df_sim

# --- 2Ô∏è‚É£ Colonnes par type ---
numeric_features = ["num_diplomas", "num_experiences", "total_experience_years", "mean_past_rating"]
categorical_features = ["city", "highest_degree", "highest_degree_domain"]
text_features = ["description", "course_title"]

# --- 3Ô∏è‚É£ Pipelines de transformation ---
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="Inconnu")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# On remplace les NaN textuels par "Inconnu"
def fill_text_unknowns(df):
    for col in text_features:
        df[col] = df[col].fillna("Inconnu")
        df[col] = df[col].replace("", "Inconnu")
    return df

# --- 4Ô∏è‚É£ Pr√©processeur global ---
preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, numeric_features),
    ("cat", categorical_transformer, categorical_features),
    ("desc_tfidf", TfidfVectorizer(max_features=300, stop_words=french_stopwords), "description"),
    ("title_tfidf", TfidfVectorizer(max_features=300, stop_words=french_stopwords), "course_title")
])

# --- 5Ô∏è‚É£ Pipeline complet ---
model_robust = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", RandomForestRegressor(
        n_estimators=150,
        random_state=42,
        n_jobs=-1
    ))
])

# --- 6Ô∏è‚É£ Pr√©paration des donn√©es ---
X_train_aug = simulate_unknowns(X_train, p=0.15)
X_train_aug = fill_text_unknowns(X_train_aug)
X_test = fill_text_unknowns(X_test)

# --- 7Ô∏è‚É£ Entra√Ænement ---
model_robust.fit(X_train_aug, y_train)

# --- 8Ô∏è‚É£ √âvaluation ---
y_pred = model_robust.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"‚úÖ MAE (mod√®le robuste) : {mae:.3f}")
print(f"‚úÖ RMSE (mod√®le robuste) : {rmse:.3f}")

# --- 9Ô∏è‚É£ Comparaison √©chantillon ---
comparison = pd.DataFrame({
    "Vraie note": y_test.values[:10],
    "Pr√©diction": np.round(y_pred[:10], 2)
})
display(comparison)

# ======================================================
#  üîç Importance des variables - mod√®le robuste
# ======================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Extraire le mod√®le et le preprocess
rf = model_robust.named_steps["regressor"]
preprocessor = model_robust.named_steps["preprocessor"]

# 1Ô∏è‚É£ R√©cup√©rer les noms des features
# --- num√©riques ---
num_features = preprocessor.transformers_[0][2]

# --- cat√©gorielles (avec one-hot) ---
cat_encoder = preprocessor.transformers_[1][1].named_steps["onehot"]
cat_features = list(cat_encoder.get_feature_names_out(preprocessor.transformers_[1][2]))

# --- textes : on ne les d√©taille pas individuellement (trop nombreux), mais on les regroupe
text_features = ["description (TF-IDF)", "course_title (TF-IDF)"]

# 2Ô∏è‚É£ Fusionner les noms (num√©riques + cat√©goriques)
feature_names = list(num_features) + cat_features + text_features

# 3Ô∏è‚É£ R√©cup√©rer les importances du RandomForest
importances = rf.feature_importances_[:len(feature_names)]

# 4Ô∏è‚É£ Construire un DataFrame propre
feat_importances = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

# 5Ô∏è‚É£ Regrouper les tr√®s nombreux dummy features
feat_importances["Feature_grouped"] = feat_importances["Feature"].apply(
    lambda x: x.split("_")[0] if "_" in x else x
)
grouped = feat_importances.groupby("Feature_grouped")["Importance"].sum().reset_index()
grouped = grouped.sort_values("Importance", ascending=False)

# 6Ô∏è‚É£ Afficher les 15 plus influentes
plt.figure(figsize=(8, 5))
sns.barplot(data=grouped.head(15), x="Importance", y="Feature_grouped", palette="mako")
plt.title("Top 15 des variables les plus importantes (mod√®le robuste)")
plt.show()

# 7Ô∏è‚É£ Affichage tabulaire d√©taill√©
display(grouped.head(20))

# ======================================================
#  üß™ Mod√®le prospectif (aucune note moyenne utilis√©e)
# ======================================================

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from nltk.corpus import stopwords
import numpy as np
import pandas as pd
import nltk

nltk.download("stopwords")
french_stopwords = stopwords.words("french")

# --- Colonnes sans mean_past_rating ---
numeric_features = ["num_diplomas", "num_experiences", "total_experience_years"]
categorical_features = ["city", "highest_degree", "highest_degree_domain"]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="Inconnu")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, numeric_features),
    ("cat", categorical_transformer, categorical_features),
    ("desc_tfidf", TfidfVectorizer(max_features=300, stop_words=french_stopwords), "description"),
    ("title_tfidf", TfidfVectorizer(max_features=300, stop_words=french_stopwords), "course_title")
])

model_prospectif = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", RandomForestRegressor(
        n_estimators=200,
        random_state=42,
        n_jobs=-1
    ))
])

# --- Entra√Ænement ---
model_prospectif.fit(X_train, y_train)
y_pred_prospectif = model_prospectif.predict(X_test)

mae = mean_absolute_error(y_test, y_pred_prospectif)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_prospectif))

print(f"‚úÖ MAE (sans mean_past_rating) : {mae:.3f}")
print(f"‚úÖ RMSE (sans mean_past_rating) : {rmse:.3f}")

# ======================================================
#  üîç Importance des variables - mod√®le robuste
# ======================================================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Extraire le mod√®le et le preprocess
rf = model_prospectif.named_steps["regressor"]
preprocessor = model_prospectif.named_steps["preprocessor"]

# 1Ô∏è‚É£ R√©cup√©rer les noms des features
# --- num√©riques ---
num_features = preprocessor.transformers_[0][2]

# --- cat√©gorielles (avec one-hot) ---
cat_encoder = preprocessor.transformers_[1][1].named_steps["onehot"]
cat_features = list(cat_encoder.get_feature_names_out(preprocessor.transformers_[1][2]))

# --- textes : on ne les d√©taille pas individuellement (trop nombreux), mais on les regroupe
text_features = ["description (TF-IDF)", "course_title (TF-IDF)"]

# 2Ô∏è‚É£ Fusionner les noms (num√©riques + cat√©goriques)
feature_names = list(num_features) + cat_features + text_features

# 3Ô∏è‚É£ R√©cup√©rer les importances du RandomForest
importances = rf.feature_importances_[:len(feature_names)]

# 4Ô∏è‚É£ Construire un DataFrame propre
feat_importances = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

# 5Ô∏è‚É£ Regrouper les tr√®s nombreux dummy features
feat_importances["Feature_grouped"] = feat_importances["Feature"].apply(
    lambda x: x.split("_")[0] if "_" in x else x
)
grouped = feat_importances.groupby("Feature_grouped")["Importance"].sum().reset_index()
grouped = grouped.sort_values("Importance", ascending=False)

# 6Ô∏è‚É£ Afficher les 15 plus influentes
plt.figure(figsize=(8, 5))
sns.barplot(data=grouped.head(15), x="Importance", y="Feature_grouped", palette="mako")
plt.title("Top 15 des variables les plus importantes (mod√®le robuste)")
plt.show()

# 7Ô∏è‚É£ Affichage tabulaire d√©taill√©
display(grouped.head(20))

# ======================================================
#  üíæ Sauvegarde des mod√®les - Robuste & Prospectif
# ======================================================

import joblib

# Sauvegarde du mod√®le robuste (avec "Inconnu" et mean_past_rating)
joblib.dump(model_robust, "model_robust.pkl")
print("‚úÖ Mod√®le robuste sauvegard√© : model_robust.pkl")

# Sauvegarde du mod√®le prospectif (aucune note historique)
joblib.dump(model_prospectif, "model_prospectif.pkl")
print("‚úÖ Mod√®le prospectif sauvegard√© : model_prospectif.pkl")

# ======================================================
#  üìä Analyse comparative des deux mod√®les (Robuste vs Prospectif)
# ======================================================

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error

# --- 1Ô∏è‚É£ Pr√©dictions des deux mod√®les sur le m√™me X_test ---
y_pred_robust = model_robust.predict(X_test)
y_pred_prospectif = model_prospectif.predict(X_test)

mae_robust = mean_absolute_error(y_test, y_pred_robust)
rmse_robust = np.sqrt(mean_squared_error(y_test, y_pred_robust))

mae_prosp = mean_absolute_error(y_test, y_pred_prospectif)
rmse_prosp = np.sqrt(mean_squared_error(y_test, y_pred_prospectif))

# --- 2Ô∏è‚É£ R√©sum√© global ---
results = pd.DataFrame({
    "Mod√®le": ["Robuste (avec notes)", "Prospectif (sans notes)"],
    "MAE": [mae_robust, mae_prosp],
    "RMSE": [rmse_robust, rmse_prosp]
})
print("üìä Comparaison des erreurs globales :")
display(results)

# --- 3Ô∏è‚É£ Distribution des erreurs ---
errors_robust = y_test - y_pred_robust
errors_prosp = y_test - y_pred_prospectif

plt.figure(figsize=(8,4))
sns.kdeplot(errors_robust, fill=True, label="Robuste")
sns.kdeplot(errors_prosp, fill=True, label="Prospectif")
plt.axvline(0, color="red", linestyle="--")
plt.title("Distribution des erreurs de pr√©diction")
plt.xlabel("Erreur (note r√©elle - pr√©dite)")
plt.legend()
plt.show()

# --- 4Ô∏è‚É£ Importance des variables pour chaque mod√®le ---
def get_feature_importances(model, model_name):
    rf = model.named_steps["regressor"]
    preprocessor = model.named_steps["preprocessor"]

    num_features = preprocessor.transformers_[0][2]
    cat_encoder = preprocessor.transformers_[1][1].named_steps["onehot"]
    cat_features = list(cat_encoder.get_feature_names_out(preprocessor.transformers_[1][2]))
    text_features = ["description (TF-IDF)", "course_title (TF-IDF)"]
    feature_names = list(num_features) + cat_features + text_features

    importances = rf.feature_importances_[:len(feature_names)]

    feat_df = pd.DataFrame({
        "Feature": feature_names,
        "Importance": importances,
        "Model": model_name
    })
    feat_df["Feature_grouped"] = feat_df["Feature"].apply(
        lambda x: x.split("_")[0] if "_" in x else x
    )
    grouped = feat_df.groupby(["Model", "Feature_grouped"])["Importance"].sum().reset_index()
    return grouped

feat_robust = get_feature_importances(model_robust, "Robuste")
feat_prosp = get_feature_importances(model_prospectif, "Prospectif")

# --- Fusion pour comparaison ---
feat_compare = pd.concat([feat_robust, feat_prosp])

plt.figure(figsize=(9,6))
sns.barplot(data=feat_compare, x="Importance", y="Feature_grouped", hue="Model")
plt.title("Comparaison de l'importance des variables (Robuste vs Prospectif)")
plt.show()

# --- 5Ô∏è‚É£ Aper√ßu tabulaire ---
feat_pivot = feat_compare.pivot(index="Feature_grouped", columns="Model", values="Importance").fillna(0)
feat_pivot["Diff√©rence"] = feat_pivot["Robuste"] - feat_pivot["Prospectif"]
display(feat_pivot.sort_values("Diff√©rence", ascending=False))

# ======================================================
#  üß™ Test local d'une pr√©diction - Professeur & Cours
# ======================================================

import pandas as pd

# --- Exemple 1 : professeur avec historique connu (mod√®le robuste) ---
prof_robuste = {
    "city": "Paris",
    "highest_degree": "Master",
    "highest_degree_domain": "Informatique",
    "num_diplomas": 2,
    "num_experiences": 4,
    "total_experience_years": 6,
    "mean_past_rating": 4.6,  # ‚úÖ connu
    "description": "Formateur exp√©riment√© en data science et IA appliqu√©e.",
    "course_title": "Machine Learning avanc√©"
}

# --- Exemple 2 : nouveau professeur sans historique (mod√®le prospectif) ---
prof_prospectif = {
    "city": "Paris",
    "highest_degree": "Doctorat",
    "highest_degree_domain": None,
    "num_diplomas": None,
    "num_experiences": None,
    "total_experience_years": 6,
    "mean_past_rating": None,  # ‚úÖ connu
    "description": "N",
    "course_title": "Web"
}


# --- Convertir en DataFrame ---
df_robuste = pd.DataFrame([prof_robuste])
df_prospectif = pd.DataFrame([prof_prospectif])

# --- Pr√©diction avec le mod√®le appropri√© ---
pred_robuste = model_robust.predict(df_robuste)[0]
pred_prospectif = model_prospectif.predict(df_prospectif)[0]

# --- R√©sultats propres ---
print("üßë‚Äçüè´ Professeur AVEC historique")
print(f"‚Üí Pr√©diction (mod√®le robuste) : {round(float(pred_robuste), 2)} / 5\n")

print("üë©‚Äçüè´ Nouveau professeur (SANS historique)")
print(f"‚Üí Pr√©diction (mod√®le prospectif) : {round(float(pred_prospectif), 2)} / 5")